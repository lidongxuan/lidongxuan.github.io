---
layout: post
title: 机器学习：一些常见的监督型学习方法（K近邻、决策树、朴素贝叶斯、逻辑回归）
comments: false
---

<!--more-->

在机器学习中，无监督学习（Unsupervised learning）就是聚类，事先不知道样本的类别，通过某种办法，把相似的样本放在一起归位一类；而监督型学习（Supervised learning）就是有训练样本，带有属性标签，也可以理解成样本有输入有输出。

所有的回归算法和分类算法都属于监督学习。回归（Regression）和分类（Classification）的算法区别在于输出变量的类型，定量输出称为回归，或者说是连续变量预测；定性输出称为分类，或者说是离散变量预测。

以下是一些常用的监督型学习方法。

### 一. K-近邻算法（k-Nearest Neighbors，KNN）###

K-近邻是一种分类算法，其思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。K通常是不大于20的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。

![knn](/public/images/2017-1-12-SvAlgorithm1/knn.jpg)

如上图，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类。

算法的步骤为：

（1）计算测试数据与各个训练数据之间的距离；

（2）按照距离的递增关系进行排序；

（3）选取距离最小的K个点；

（4）确定前K个点所在类别的出现频率；

（5）返回前K个点中出现频率最高的类别作为测试数据的预测分类。

### 二. 决策树（Decision Trees） ###

决策树是一种常见的分类方法，其思想和“人类逐步分析比较然后作出结论”的过程十分相似。决策过程和下图类似。

![decisiontree](/public/images/2017-1-12-SvAlgorithm1/decisiontree.png)

决策树是一个树结构（可以是二叉树或非二叉树）。其每个非叶节点表示一个特征属性上的测试，每个分支代表这个特征属性在某个值域上的输出，而每个叶节点存放一个类别。使用决策树进行决策的过程就是从根节点开始，测试待分类项中相应的特征属性，并按照其值选择输出分支，直到到达叶子节点，将叶子节点存放的类别作为决策结果。

不同于贝叶斯算法，决策树的构造过程不依赖领域知识，它使用属性选择度量来选择将元组最好地划分成不同的类的属性。所谓决策树的构造就是进行属性选择度量确定各个特征属性之间的拓扑结构。

那么如何划分数据呢？各个特征的优先级是怎么排的？常用的划分数据集方法有ID3和C4.5

**（1） [ID3](https://en.wikipedia.org/wiki/ID3_algorithm)算法**

划分数据集的最大原则就是将数据变得更加有序。**熵（entropy）**是描述信息不确定性（杂乱程度）的一个值。设$S$是当前数据下的划分，那么$S$的**信息熵**的定义如下：

$$H(S)=-\sum_{i=1}^{n}p(x_i)\log_{2}p(x_i)$$

这里，$n$是类别的数目，$p(x_i)$表示选择$x_i$类别的概率（可用类别数量除以总数量估计）。

现在我们假设将$S$按属性$A$进行划分，则$S$的**条件信息熵**（$A$对$S$划分的期望信息）为：

$$H(S|A)=\sum_{j=1}^{m}p(t_j)H(t_j)$$

这里，在属性$A$的条件下，数据被划分成$m$个类别（例如，属性A是体重，有轻、中、重三个选项，那么$m=3$），$p(t_j)$表示类别$t_j$（属性A中所有具有第j个特性的所有数据）的数量与$S$总数量的比值，$H(t_j)$表示子类别$t_j$的熵。

**信息增益（Information gain）**是指在划分数据集之前之后信息发生的变化，其定义如下：

$$IG(S,A)=H(S)-H(S|A)=H(S)-\sum_{j=1}^{m}p(t_j)H(t_j)$$

在ID3算法里，每一次迭代过程中会计算所有剩余属性的信息增益，然后选择具有最大增益的属性对数据集进行划分，如此迭代，直至结束。[这里有一个ID3算法的实例过程](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)。

**(2) [C4.5](https://en.wikipedia.org/wiki/C4.5_algorithm)算法**

D3算法存在一个问题，就是偏向于多值属性，例如，如果存在唯一标识属性ID，则ID3会选择它作为分裂属性，这样虽然使得划分充分纯净，但这种划分对分类几乎毫无用处。ID3的后继算法C4.5使用增益率（gain ratio）的信息增益扩充，试图克服这个偏倚。严格上说C4.5是ID3的一个改进算法。

在按照ID3的中的方法得到了信息增益后，再定义**分裂信息（Split Information）**：

$$SI(S,A)=-\sum_{j=1}^{m}\frac{|t_j|}{|S|}\log_{2}\frac{|t_j|}{|S|}$$

然后定义**增益率（Gain Ratio）**:

$$GR(S,A)=\frac{IG(S,A)}{SI(S,A)}$$

C4.5选择增益率为分裂属性（连续属性要用增益率离散化）。C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。

如果所有属性都作为分裂属性用光了，但有的子集还不是纯净集，即集合内的元素不属于同一类别。在这种情况下，由于没有更多信息可以使用了，一般对这些子集进行“多数表决”，即使用此子集中出现次数最多的类别作为此节点类别，然后将此节点作为叶子节点。

在实际构造决策树时，通常要进行剪枝，这时为了处理由于数据中的噪声和离群点导致的过分拟合问题。剪枝有两种：先剪枝——在构造过程中，当某个节点满足剪枝条件，则直接停止此分支的构造；后剪枝——先构造完成完整的决策树，再通过某些条件遍历树进行剪枝。悲观错误剪枝PEP算法是一种常见的事后剪枝策略。

### 三. 朴素贝叶斯（Naive Bayesian）###

贝叶斯分类是一系列分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。朴素贝叶斯算法（Naive Bayesian) 是其中应用最为广泛的分类算法之一。朴素贝叶斯分类器基于一个简单的假定：**给定目标值时属性之间相互条件独立**。朴素贝叶斯的基本思想是对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

首先给出**条件概率**的定义，$P(A\|B)$表示事件A在B发生下的条件概率，其公式为：

$$P(A|B)=\frac{P(AB)}{P(B)}$$

**贝叶斯定理**用来描述两个条件概率之间的关系，贝叶斯定理公式为：

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

朴素贝叶斯分类算法的具体步骤如下：

（1）设$x=\{a_1,a_2,...,a_m\}$为一个待分类项，$a_1,a_2,...,a_m$为$x$的$m$个特征属性;

（2）设有类别集合$C=\{y_1,y_2,...,y_n\}$，即共有$n$个类别；

（3）依次计算$x$属于各项分类的条件概率，即计算$P(y_1\|x)$，$P(y_2\|x)$，... ，$P(y_n\|x)$：

$$P(y_1|x)=\frac{P(x|y_1)P(y_1)}{P(x)}=\frac{P(y_1)}{P(x)}\prod_{i=1}^{m}P(a_i|y_1)$$
<center> ...</center >
$$P(y_n|x)=\frac{P(x|y_n)P(y_n)}{P(x)}=\frac{P(y_n)}{P(x)}\prod_{i=1}^{m}P(a_i|y_n)$$

注意，算法的下一步骤是对比这些结果的大小，由于各项分母都是$P(x)$，所以分母不用计算。分子部分中$P(y_n)$和$P(a_i\|y_n)$都是通过样本集统计而得，其中$P(y_n)$的值为样本集中属于$y_n$类的数量与样本总数量之比，$P(a_i\|y_n)$的值为$y_n$类中满足属性$a_i$的数量与$y_n$类下样本总数量之比。

这样的计算方式符合特征属性是离散值的情况，如果特征属性是连续值时，通常假定其值服从高斯分布（也称正态分布），即$g(x,\eta,\sigma)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\eta)^2}{2\sigma^2}}$，那么$P(a_i\|y_n)$的值为：

$$P(a_i|y_n)=g(a_i,\eta_{y_n},\sigma_{y_n})=\frac{1}{\sqrt{2\pi}\sigma_{y_n}}e^{-\frac{(x-\eta_{y_n})^2}{2\sigma_{y_n}^2}}$$

其中，$\eta_{y_n}$和$\sigma_{y_n}$分别为训练样本$y_n$类别中$a_i$特征项划分的均值和标准差。

对于$P(a\|y)=0$的情况，当某个类别下某个特征项划分没有出现时，就是产生这种现象，这会令分类器质量大大降低。因此引入Laplace校准，对没类别下所有划分的计数加1，这样如果训练样本集数量充分大时，并不会对结果产生影响，也避免了乘积为0的情况。

（4）比较（3）中所有条件概率的大小，最大的即为预测分类结果，即：

$$P(y_k|x)=\max\{P(y_1|x),P(y_2|x),...,P(y_n|x)\}$$

$$x\in y_k$$

这里有一个朴素贝叶斯分类实例：[检测SNS社区中不真实账号](http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html)。

### 四. 逻辑回归（Logistic Regression） ###

我们知道，线性回归就是根据已知数据集求一线性函数，使其尽可能拟合数据，让损失函数最小，常用的线性回归最优法有[最小二乘法](https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Motivational_example)和[梯度下降法](http://blog.csdn.net/woxincd/article/details/7040944)。而逻辑回归是一种非线性回归模型，相比于线性回归，它多了一个sigmoid函数（或称为Logistic函数）。逻辑回归是一种分类算法，主要用于二分类问题。逻辑回归的具体步骤如下：

**（1）定义假设函数h（即hypothesis）**

Sigmoid函数的图像是一个S型，预测函数就是将sigmoid函数$g(x)$里的自变量$x$替换成了边界函数$\theta(x)$，如下：

$$h_\theta(x)=g[\theta(x)]=\frac{1}{1+e^{-\theta(x)}}$$

这里$h_\theta(x)$表示结果取1的概率，因此对于输入$x$分类结果为类别1和类别0的概率分别为：

$$P(y=1|x,\theta)=h_\theta(x)$$

$$P(y=0|x,\theta)=1-h_\theta(x)$$

**(2)定义边界函数$\theta(x)$**

对于二维数据，如果是线性线性边界，那么边界函数为：

$$\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2$$

如果是非线性线性边界，那么边界函数为的形式就多了，例如：

$$\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1^2+\theta_4x_2^2$$

**(3)构造损失函数（cost function，loss function）**

损失函数的大小可以体现出边界函数的各项参数是否最优。对于线性回归，损失函数是欧式距离指标，但这样的Cost Function对于逻辑回归是不可行的，因为在逻辑回归中平方差损失函数是非凸，我们需要其他形式的Cost Function来保证逻辑回归的成本函数是凸函数。

我们选择对数似然损失函数:

$$Cost(h_{\theta}(x),y)=\begin{cases} -\log h_{\theta}(x),\ & \textrm{if} \ y=1\\ -\log (1-h_{\theta}(x)),\ & \textrm{if} \ y=0\end{cases}$$

那么逻辑回归的Cost Function可以表示为：

$$\begin{align}J(\theta) & =\frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}(x^{(i)}),y^{(i)})\\ & =-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}\log h_{\theta}(x^{(i)})+(1-y^{(i)})\log (1-h_{\theta}(x^{(i)}))]\end{align}$$

这里$m$表示有m个样本，$y$是二值型数据，只能0或1，代表两种不同的类别。

**(4)求最优$\theta$**

要想找到最合适的边界函数参数，只要使$J(\theta)$最小即可。最优化的表达式为：

$$\theta=\mathop{\textrm{argmin}}_{\theta} J(\theta)$$

与线性回归相似，可以采用梯度下降法寻优，也可以采用其他方法，具体见第5个参考网址。

## 参考资料 ##

[机器学习（一）K-近邻（KNN）算法](http://www.cnblogs.com/ybjourney/p/4702562.html)

[算法杂货铺——分类算法之决策树(Decision tree)](http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html)

[决策树算法总结](http://www.cnblogs.com/biyeymyhjob/archive/2012/07/23/2605208.html)

[算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)](http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html)

[Coursera公开课笔记: 斯坦福大学机器学习第六课“逻辑回归(Logistic Regression)”](http://52opencourse.com/125/coursera公开课笔记-斯坦福大学机器学习第六课-逻辑回归-logistic-regression)
