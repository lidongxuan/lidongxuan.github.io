---
layout: post
title: 机器学习：线性回归、局部加权线性回归、岭回归、前向逐步回归
comments: false
---

<!--more-->

所谓回归，简单来讲就是根据现有数据拟合出函数，然后根据该函数进行一些预测工作。分类的输出是标称型，而回归的输出为数值型。接下来介绍几种常见的全局型回归方法

### 一. 线性回归（Linear Regression） ###

对于二维数据而言，线性回归就是找出一个一次函数去拟合数据，使得平方误差最小。是的，这里的损失函数是平方损失。平方误差可以写做：

$$\sum^m_i=(y_i-x_i^Tw)^2$$

矩阵写法为：

$$(y-Xw)^T(y-Xw)$$

对$w$求导并令其等于0，于是解出$w$如下：

$$w'=(X^TX)^{-1}X^Ty$$

$w'$即为最优解，这就是“普通最小二乘法”，当然在应用此算法之前需要的判断$X^TX$的可逆性。如果不可逆，那么此办法就不可用，应该用岭回归算法，具体在第三部分讨论。

### 二. 局部加权线性回归（Locally Weighted Linear Regression） ###

我们知道并不是所有数据都能用线性回归来拟合，有的时候数据有转折或是其他规律。于是有了局部加权线性回归，该方法更加关注将要预测的数据周围的点，也就是通过高斯核给预测点周围的数据赋予权值，选择合适的高斯系数能使结果更好，当然这种算法的缺点是：对于每一个要预测的点，都要重新依据整个数据集计算一个线性回归模型出来，使得算法代价极高。

该算法的详细介绍有一篇blog写得很好————[【机器学习】局部加权线性回归](http://blog.csdn.net/herosofearth/article/details/51969517)

### 三. 岭回归（Ridge Regression） ###

岭回归是一种常见的shrinkage coefficient（缩减系数）方法，还有其他缩减方法，如lasso（效果很好但计算复杂）、LAR、PCA。所谓shrinkage，就是通过一些手段来减少不重要的参数。

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于普通最小二乘法。那么什么样得数据算是病态数据，主要有两种，一种是数据点少于变量数（特征维数），这种情况下特征信息矩阵维数无法求逆，不满足最小二乘得条件；第二种是自变量中的某几个存在多重共线性，对于这样得数据虽然可得到无偏解，但是却有可能和实际情况相差甚远，[一个例子](https://www.zhihu.com/question/28221429/answer/53858036)。

岭回归对原有的最优化问题加了一个二次正则项，也称L2正则化，如下：

$$\min \sum ||y-X\beta||^2+\lambda \sum||\beta||^2$$

其中$\lambda$为正则化参数，我们知道在普通最小二乘法里有一个基本要求是$X^TX$可逆，对$X^TX$加上了一个$\lambda I$后刚好可逆，此时回归系数的计算公式为：

$$w'=(X^TX+\lambda I)^{-1}X^Ty$$

当$\lambda$趋于0时，回归系数$w$的值和普通线性回归得到的一致，当$\lambda$趋于无穷大时，回归系数接近于0，可以通过交叉验证的方法选择出使得测试结果最好的$\lambda$。

在lasso算法里，约束条件变成了绝对值而不是二次方，也就是L1正则化，计算程度比较复杂。

### 四. 前向逐步回归（Foward Stagewise Regression） ###

前向逐步回归算法可以得到和lasso差不多的效果，但更简单，是一种贪心算法。一开始，所有的权重都设置为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值，算法伪代码如下：

```
数据标准化，使其分布满足0均值和单位方差 
在每轮迭代过程中:
	设置当前最小误差lowestError为正无穷
	对每个特征: 
		增大或缩小: 
			改变一个系数得到一个新的w
			计算新W下的误差 
			如果误差Error小于当前最小误差lowestError:
				设置Wbest等于当前的W
		将1^设置为新的Wbest
```



### 五. 参考 ###

[【机器学习】局部加权线性回归](http://blog.csdn.net/herosofearth/article/details/51969517)

[《机器学习实战》]()

